{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM and Shock Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCnXH2T4oP6P"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFYKjm6lodo-"
      },
      "source": [
        "# Generic\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy import signal\n",
        "from scipy.fft import fftshift\n",
        "import scipy\n",
        "from datetime import datetime\n",
        "\n",
        "#ML Libs\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import scipy.ndimage as nd\n",
        "from keras.models import Sequential,load_model\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras import metrics\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Convolution2D, MaxPooling2D, BatchNormalization, ReLU, LeakyReLU, Conv2D, LSTM, LeakyReLU\n",
        "from keras import layers\n",
        "from keras.optimizers import adam_v2, adagrad_v2, rmsprop_v2\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, auc, roc_curve, precision_recall_fscore_support, mean_squared_error\n",
        "#from keras.layers.experimental import preprocessing\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "\n",
        "Terminal = False\n",
        "\n",
        "#Loading SQI Matched with clinitcal to the Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4m2gx-Kptde"
      },
      "source": [
        "#Loading SQI Matched with clinical to the Dataframe\n",
        "\n",
        "id = '1DDf_vluJE-zg--41A0zpP5_9-FU-wsJp' # The shareable link id\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Complete_SQIs_with_Clinical.csv')  \n",
        "SQI_C = pd.read_csv('Complete_SQIs_with_Clinical.csv')\n",
        "\n",
        "\n",
        "id = '1zEpiX0yXT16cz3znpo8wjoV_U_8vSoyb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Raw_signals.csv')  \n",
        "Raw = pd.read_csv('Raw_signals.csv')\n",
        "\n",
        "if Terminal:\n",
        "    print(\"\\n SQI with Clinical Match:\")\n",
        "    print(SQI_C)\n",
        "    print(\"\\n Raw Signals\")\n",
        "    print(Raw)\n",
        "\n",
        "\n",
        "#Not really need, here for completeness\n",
        "event = ['event_shock', 'reshock24','diagnosis_admission',\\\n",
        "     'ascites', 'respiratory_distress', 'ventilation_cannula', \\\n",
        "     'ventilation_mechanical', 'ventilation_ncpap', 'bleeding_severe', \\\n",
        "     'cns_abnormal', 'liver_mild', 'pleural_effusion', 'skidney']\n",
        "\n",
        "event_shock = 'shock_admission'\n",
        "\n",
        "SQI_C['keep'] = False\n",
        "for i in range(len(event)):\n",
        "    event_s = event[i]\n",
        "    SQI_C['keep'][SQI_C[event_s] == True] = True\n",
        "    #print(\"\\n Total \", event[i], \" events:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0bn91v2qFJs"
      },
      "source": [
        "np.random.seed(7)\n",
        "#Patient 1 pre shock (Admitted with Shock)\n",
        "#Patient 2 post shock, in this case P1 and P2 are the same 2009\n",
        "#Unfortunately Post shock data is much shorter and therefore we split it into smaller windows (Data is still less)\n",
        "Terminal = False\n",
        "if Terminal:\n",
        "  #Print the Study\n",
        "  print('Study 003-2009 Datetime:')\n",
        "  print(Raw[(Raw.study_no == '003-2009')].PPG_Datetime)\n",
        "\n",
        "  #Printing Time of Event\n",
        "  print('PPG Shock Event Start -2009:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2009') & (SQI_C.keep == True)].PPG_w_s)\n",
        "  print('PPG Shock Event End -2009:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2009') & (SQI_C.keep == True)].PPG_w_f)\n",
        "\n",
        "  #Print the Study\n",
        "  print('Study 003-2103 Datetime:')\n",
        "  print(Raw[(Raw.study_no == '003-2103')].PPG_Datetime)\n",
        "\n",
        "  #Printing Time of Event\n",
        "  print('PPG Shock Event Start -2103:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2103') & (SQI_C.keep == True)].PPG_w_s)\n",
        "  print('PPG Shock Event End -2103:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2103') & (SQI_C.keep == True)].PPG_w_f)\n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "P1times = datetime.strptime('28072020160400000', '%d%m%Y%H%M%S%f')########## Start Until Shock\n",
        "P1timef = datetime.strptime('29072020032820200', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P2times = datetime.strptime('29072020033146280', '%d%m%Y%H%M%S%f')########## Shock Until the End\n",
        "P2timef = datetime.strptime('18092020044800000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "# P3times = datetime.strptime('29072020033146280', '%d%m%Y%H%M%S%f')########## Shock Until the End for Patient 2103\n",
        "# P3timef = datetime.strptime('18092020044800000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "#Extracting\n",
        "Patient1 = Raw[(Raw.study_no == '003-2009') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2009') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "#Patient3 = Raw[(Raw.study_no == '003-2103') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "\n",
        "\n",
        "#Unnecessary Label Added\n",
        "Patient1['Label'] = 0 \n",
        "Patient2['Label'] = 1\n",
        "#Patient3['Label'] = 1\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 684) #Split into 60 Sec Windows\n",
        "Patient2_w = np.array_split(Patient2, 77)\n",
        "#Patient3_w = np.array_split(Patient3, 50)##########\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),6000,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'PLETH_bpf') #Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'PLETH_bpf')\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((684,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((77,1), dtype=int)\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr,Patient2_w_arr))\n",
        "Labels = np.vstack((Patient1_w_arr_l,Patient2_w_arr_l))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zLuUhKn-K_F"
      },
      "source": [
        "Data1 = np.vstack((Patient1_w_arr[-77:,:],Patient2_w_arr)) #for a balanced model we use slicing \n",
        "Labels1 = np.vstack((Patient1_w_arr_l[-77:,:],Patient2_w_arr_l))\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Data1, Labels1, stratify=Labels1)\n",
        "\n",
        "# if Terminal:\n",
        "#   print(train_index)\n",
        "#   print(test_index)\n",
        "#Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "#Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "Globalmean = np.mean(X_train)\n",
        "Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "P_train = (X_train - Globalmean) / Globalstd\n",
        "P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "#Empty arrays\n",
        "#P1Sxx = np.empty((len(P1_training),26,32))\n",
        "#P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "PSxx = np.empty((len(P_train),26,26))\n",
        "PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "\n",
        "#Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "#Patient 1\n",
        "\n",
        "def spectrogram_arr(Input, Output_arr, fs):\n",
        "  for i in range(len(Input)):\n",
        "    f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "    fmin = 0 # Hz\n",
        "    fmax = 10 # Hz\n",
        "    freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "    # keep only frequencies of interest\n",
        "    f   = f[freq_slice]\n",
        "    Sxx = Sxx[freq_slice,:][0]\n",
        "    Output_arr[i] = Sxx\n",
        "  return Output_arr\n",
        "\n",
        "X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "Terminal = False\n",
        "\n",
        "if Terminal:\n",
        "  print(X_train.shape)\n",
        "  print(y_train.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "#MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "#tf.random.set_seed(1234)\n",
        "\n",
        "#converting to categorical\n",
        "y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "#X_trainf = X_train.reshape(-1, 26*32)\n",
        "#X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# #print(X_train)\n",
        "# print(X_test.shape)\n",
        "# pca = PCA(n_components=26) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "# for i in range(X_train.shape[2]):\n",
        "#   pca = pca.fit(X_train[:,:,i])\n",
        "#   X_trainf = pca.transform(X_train[:,:,i])\n",
        "#   X_testf = pca.transform(X_test[:,:,i])\n",
        "\n",
        "\n",
        "pca = PCA(n_components=80) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "pca = pca.fit(X_train_2d)\n",
        "X_trainf = pca.transform(X_train_2d)\n",
        "X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "\n",
        "Terminal = True\n",
        "print(X_trainf.shape)\n",
        "print(X_testf.shape)\n",
        "\n",
        "if Terminal:\n",
        "  print('Train Labels')\n",
        "  print(y_trainf.shape)\n",
        "  print('Train Data')\n",
        "  print(X_train.shape)\n",
        "\n",
        "# Simple ANN Start ========================================================================================\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256,'relu', input_shape=(80,)))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(512,'relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256,'relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('Softmax'))\n",
        "\n",
        "# . . . \n",
        "#early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "history = model.fit(X_trainf, y_trainf, epochs=300, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "# ANN ==========================================================================================\n",
        "\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Accuracy and Loss')\n",
        "plt.ylabel('accuracy/loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "y_prob= model.predict(X_testf)\n",
        "y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "print('\\nConfusion Matrix on Unseen Data:')\n",
        "print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "print('\\nReport on Unseen Data:')\n",
        "print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "  #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "  # probs = model.predict_proba(X_testf)\n",
        "  # preds = probs[:,1]\n",
        "  # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "  # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "  # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  # plt.legend(loc = 'lower right')\n",
        "  # plt.plot([0, 1], [0, 1],'r--')\n",
        "  # plt.xlim([0, 1])\n",
        "  # plt.ylim([0, 1])\n",
        "  # plt.ylabel('True Positive Rate')\n",
        "  # plt.xlabel('False Positive Rate')\n",
        "  # plt.show()\n",
        "\n",
        "#   Score_array[counter] = score[1]\n",
        "#   Recall[counter] = PRF[1]\n",
        "#   Precision[counter] = PRF[0] \n",
        "#   F1S[counter] =  PRF[2]\n",
        "#   counter = counter + 1\n",
        "\n",
        "# print(Score_array)\n",
        "\n",
        "# print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "# print(np.max(Score_array))\n",
        "\n",
        "# print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "# print(np.min(Score_array))\n",
        "\n",
        "# print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "# print(np.sum(Score_array)/5)\n",
        "# print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "# print(np.sum(Recall)/5)\n",
        "# print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "# print(np.sum(Precision)/5)\n",
        "# print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "# print(np.sum(F1S)/5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvghPn91ubZE"
      },
      "source": [
        "# split into train and test sets\n",
        "train_size = int(len(Data) * 1)\n",
        "test_size = len(Data) - train_size\n",
        "Train, Test = Data[0:train_size,:], Data[train_size:len(Data),:]\n",
        "print(len(Train), len(Test))\n",
        "labels_tr, labels_te = Labels[0:train_size,:], Labels[train_size:len(Data),:]\n",
        "\n",
        "#Z-score normalization\n",
        "Globalmean = np.mean(Train)\n",
        "Globalstd = np.std(Train)\n",
        "\n",
        "P_train = (Train - Globalmean) / Globalstd\n",
        "P_test = (Test - Globalmean) / Globalstd\n",
        "\n",
        "\n",
        "#Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "  #Patient 1\n",
        "\n",
        "PSxx = np.empty((len(P_train),26,26))\n",
        "PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "fs = 100\n",
        "\n",
        "def spectrogram_arr(Input, Output_arr, fs):\n",
        "  for i in range(len(Input)):\n",
        "    f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "    fmin = 0 # Hz\n",
        "    fmax = 10 # Hz\n",
        "    freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "    # keep only frequencies of interest\n",
        "    f   = f[freq_slice]\n",
        "    Sxx = Sxx[freq_slice,:][0]\n",
        "    Output_arr[i] = Sxx\n",
        "    return Output_arr\n",
        "\n",
        "X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_test.shape)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=80) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "#X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "pca = pca.fit(X_train_2d)\n",
        "X_trainf = pca.transform(X_train_2d)\n",
        "#X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "print(X_trainf.shape)\n",
        "#print(X_testf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUKQag_4Iju"
      },
      "source": [
        "Data1 = np.vstack((Patient1_w_arr[-77:,:],Patient2_w_arr)) #for a balanced model we use slicing \n",
        "Labels1 = np.vstack((Patient1_w_arr_l[-77:,:],Patient2_w_arr_l))\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Data1, Labels1, stratify=Labels1)\n",
        "\n",
        "# if Terminal:\n",
        "#   print(train_index)\n",
        "#   print(test_index)\n",
        "#Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "#Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "Globalmean = np.mean(X_train)\n",
        "Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "P_train = (X_train - Globalmean) / Globalstd\n",
        "P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "#Empty arrays\n",
        "#P1Sxx = np.empty((len(P1_training),26,32))\n",
        "#P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "PSxx = np.empty((len(P_train),26,26))\n",
        "PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "\n",
        "#Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "#Patient 1\n",
        "\n",
        "def spectrogram_arr(Input, Output_arr, fs):\n",
        "  for i in range(len(Input)):\n",
        "    f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "    fmin = 0 # Hz\n",
        "    fmax = 10 # Hz\n",
        "    freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "    # keep only frequencies of interest\n",
        "    f   = f[freq_slice]\n",
        "    Sxx = Sxx[freq_slice,:][0]\n",
        "    Output_arr[i] = Sxx\n",
        "  return Output_arr\n",
        "\n",
        "\n",
        "#converting to categorical\n",
        "X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "Terminal = True\n",
        "\n",
        "if Terminal:\n",
        "  print(X_train.shape)\n",
        "  print(y_train.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "#MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "#tf.random.set_seed(1234)\n",
        "\n",
        "#converting to categorical\n",
        "y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "X_trainf = X_train.reshape(-1, 26*26)\n",
        "X_testf = X_test.reshape(-1, 26*26)\n",
        "\n",
        "if Terminal:\n",
        "  print('Train Labels')\n",
        "  print(y_trainf.shape)\n",
        "  print('Train Data')\n",
        "  print(X_train.shape)\n",
        "\n",
        "# Simple ANN Start ========================================================================================\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256,'relu', input_shape=(26*26,), kernel_initializer='glorot_uniform'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(512,'relu', kernel_initializer='glorot_uniform'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128,'relu', kernel_initializer='glorot_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('Softmax'))\n",
        "\n",
        "# . . . \n",
        "#early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "history = model.fit(X_trainf, y_trainf, epochs=150, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "# ANN ==========================================================================================\n",
        "\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Accuracy and Loss')\n",
        "plt.ylabel('accuracy/loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "y_prob= model.predict(X_testf)\n",
        "y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "print('\\nConfusion Matrix on Unseen Data:')\n",
        "print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "print('\\nReport on Unseen Data:')\n",
        "print(classification_report(y_test, y_pred_unseen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83cpRS6X3qNN"
      },
      "source": [
        "# split into train and test sets\n",
        "train_size = int(len(Data) * 1)\n",
        "test_size = len(Data) - train_size\n",
        "Train, Test = Data[0:train_size,:], Data[train_size:len(Data),:]\n",
        "print(len(Train), len(Test))\n",
        "labels_tr, labels_te = Labels[0:train_size,:], Labels[train_size:len(Data),:]\n",
        "\n",
        "#Z-score normalization\n",
        "Globalmean = np.mean(Train)\n",
        "Globalstd = np.std(Train)\n",
        "\n",
        "P_train = (Train - Globalmean) / Globalstd\n",
        "#P_test = (Test - Globalmean) / Globalstd\n",
        "\n",
        "\n",
        "#Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "  #Patient 1\n",
        "\n",
        "PSxx = np.empty((len(P_train),26,26))\n",
        "PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "fs = 100\n",
        "\n",
        "def spectrogram_arr(Input, Output_arr, fs):\n",
        "  for i in range(len(Input)):\n",
        "    f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "    fmin = 0 # Hz\n",
        "    fmax = 10 # Hz\n",
        "    freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "    # keep only frequencies of interest\n",
        "    f   = f[freq_slice]\n",
        "    Sxx = Sxx[freq_slice,:][0]\n",
        "    Output_arr[i] = Sxx\n",
        "    return Output_arr\n",
        "\n",
        "X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_test.shape)\n",
        "X_trainf = X_train.reshape(-1, 26*26)\n",
        "\n",
        "# pca = PCA(n_components=80) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "# X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "# #X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "# pca = pca.fit(X_train_2d)\n",
        "# X_trainf = pca.transform(X_train_2d)\n",
        "# #X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "print(X_trainf.shape)\n",
        "#print(X_testf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yvcQbJMrJSw"
      },
      "source": [
        "def LSTM_test(window, batch_sz,lstm_size,lstm_size2,lstm_size3, Epo, Drop):\n",
        "  print(\"WINDOW=======================================================\", window)\n",
        "  print(\"Batch=======================================================\", batch_sz)\n",
        "  print(\"Lstm1=======================================================\", lstm_size)\n",
        "  print(\"Lstm2=======================================================\", lstm_size2)\n",
        "  print(\"Lstm3========================================================\", lstm_size3)\n",
        "  print(\"Epochs=======================================================\", Epo)\n",
        "  print(\"Dropout======================================================\", Drop)\n",
        "  # convert an array of values into a dataset matrix\n",
        "  def create_dataset(dataset, window=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-window-1):\n",
        "      a = dataset[i:(i+window),:]\n",
        "      dataX.append(a)\n",
        "      dataY.append(dataset[i + window,:])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "  \n",
        "\n",
        "\n",
        "  #window = 26\n",
        "  train_X, train_Y = create_dataset(X_trainf, window)\n",
        "  #test_X, test_Y = create_dataset(X_test, window) ########################\n",
        "\n",
        "  print(train_X.shape)\n",
        "  print(train_Y.shape)\n",
        "  #print(test_X.shape)\n",
        "  #print(test_Y.shape)\n",
        "\n",
        "  train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 80))\n",
        "  #train_Y = np.reshape(train_Y, (train_Y.shape[0], train_Y.shape[1], 80))\n",
        "  #test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 80))\n",
        "  #batch_size = 32\n",
        "  acti = LeakyReLU(alpha=0.3)\n",
        "  return_seq = False\n",
        "  if lstm_size2 != 0 or lstm_size3 != 0:\n",
        "    return_seq = True\n",
        "\n",
        "  rnn = Sequential()    \n",
        "  rnn.add(LSTM(lstm_size,acti,return_sequences=return_seq, input_shape = train_X.shape[1:], kernel_initializer=RandomNormal(mean=0.0, stddev=0.1, seed=None)))\n",
        "  if lstm_size2 != 0:\n",
        "    rnn.add(Dropout(Drop))\n",
        "    rnn.add(LSTM(lstm_size2,acti, return_sequences=return_seq))\n",
        "  if lstm_size3 != 0:\n",
        "    rnn.add(Dropout(Drop))\n",
        "    rnn.add(LSTM(lstm_size3,acti))\n",
        "  rnn.add(Flatten())\n",
        "  rnn.add(Dense(80))\n",
        "  rnn.compile(loss = \"mean_squared_error\",  optimizer = \"rmsprop\", metrics = ['mse'])\n",
        "\n",
        "  rnn.fit(train_X, train_Y, epochs=Epo, batch_size=batch_sz, verbose=1)\n",
        "\n",
        "\n",
        "  test1 = rnn.predict(train_X)\n",
        "  #test2 = rnn.predict(test_X)\n",
        "  print(test1.shape)\n",
        "  #print(test2.shape)\n",
        "\n",
        "\n",
        "  test1 = rnn.predict(train_X)\n",
        "  #test2 = rnn.predict(test_X)\n",
        "  print('Test train and Test test shapes:')\n",
        "  print(test1.shape)\n",
        "  #print(test2.shape)\n",
        "        \n",
        "\n",
        "  predictions1 = model.predict(test1)\n",
        "  #predictions2 = model.predict(test2)\n",
        "\n",
        "  predictions_tr = np.empty((len(predictions1),1))\n",
        "  #predictions_te = np.empty((len(predictions2),1))\n",
        "\n",
        "  for i in range(len(predictions1)):\n",
        "    if predictions1[i,0] < predictions1[i,1]:\n",
        "      predictions_tr[i] = 1\n",
        "    else:\n",
        "      predictions_tr[i] = 0\n",
        "\n",
        "  # for j in range(len(predictions2)):\n",
        "  #   if predictions2[j,0] < predictions2[j,1]:\n",
        "  #     predictions_te[j] = 1\n",
        "  #   else:\n",
        "  #     predictions_te[j] = 0\n",
        "\n",
        "  rnn.summary()\n",
        "  cls1 = classification_report(labels_tr[-test1.shape[0]:],predictions_tr)#,output_dict=True)\n",
        "  #cls2 = classification_report(labels_te[-test2.shape[0]:],predictions_te,output_dict=True)\n",
        "\n",
        "  cm1 = confusion_matrix(labels_tr[-test1.shape[0]:], predictions_tr)\n",
        "  #cm2 = confusion_matrix(labels_te[-test2.shape[0]:], predictions_te)\n",
        "  \n",
        "  print(cm1)\n",
        "  print(cls1)\n",
        "\n",
        "  #vars = np.array((window, batch_sz,lstm_size,lstm_size2,lstm_size3, Epo, Drop,acti))\n",
        "\n",
        "  #var = pd.DataFrame(vars).transpose()\n",
        "  #cls = pd.DataFrame(cls1).transpose()\n",
        "  #clst = pd.DataFrame(cls2).transpose()\n",
        "\n",
        "  #Results = pd.concat((cls,clst), axis = 1)\n",
        "\n",
        "  return\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3zkWUuEb7Ph"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY66vpOEuZcf"
      },
      "source": [
        "LSTM_test(32, 80, 32, 288, 0, 300, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrzEsUI_9hVP"
      },
      "source": [
        "# acti = ['sigmoid']\n",
        "# for i in range(1):\n",
        "#   Window_tests, var = LSTM_test(5, 32,32, 0, 0, 500, 0.4, acti[i])\n",
        "#   if i == 0:\n",
        "#     Results = Window_tests\n",
        "#     Results2 = var\n",
        "#   else:\n",
        "#     Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "#     Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "# print(Results)\n",
        "# print(Results2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo1hdxTzr2jQ"
      },
      "source": [
        "for i in range(28):\n",
        "  Window_tests, var = LSTM_test(i+1, 32,32, 0, 0, 500, 0.2)\n",
        "  if i == 0:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsWindow.csv') \n",
        "files.download('ResultsWindow.csv')\n",
        "\n",
        "Results2.to_csv('VarsWindow.csv') \n",
        "files.download('VarsWindow.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGF_J0AD3RzY"
      },
      "source": [
        "for i in range(16,100,16):\n",
        "  Window_tests, var = LSTM_test(9, i,32 ,0,0, 500, 0.2)\n",
        "  if i == 16:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsBatch.csv') \n",
        "files.download('ResultsBatch.csv')\n",
        "\n",
        "Results2.to_csv('VarsBatch.csv') \n",
        "files.download('VarsBatch.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAtmw1YJKIrK"
      },
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k8XIiV-4Kk_"
      },
      "source": [
        "for i in range(32,300,32):\n",
        "  Window_tests, var = LSTM_test(9, 32, i ,0 ,0 , 500, 0.2)\n",
        "  if i == 32:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsLSTM.csv') \n",
        "files.download('ResultsLSTM.csv')\n",
        "\n",
        "Results2.to_csv('VarsLSTM.csv') \n",
        "files.download('VarsLSTM.csv')\n",
        "\n",
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my7C8hvx7w5u"
      },
      "source": [
        "for i in range(32,300,32):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , i ,0 , 250, 0.2)\n",
        "  if i == 32:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsLSTM2.csv') \n",
        "files.download('ResultsLSTM2.csv')\n",
        "\n",
        "Results2.to_csv('VarsLSTM2.csv') \n",
        "files.download('VarsLSTM2.csv')\n",
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_pnRPWl7-Ke"
      },
      "source": [
        "for i in range(32,300,32):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , 64 , i, 250, 0.2)\n",
        "  if i == 32:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsLSTM3.csv') \n",
        "files.download('ResultsLSTM3.csv')\n",
        "\n",
        "Results2.to_csv('VarsLSTM3.csv') \n",
        "files.download('VarsLSTM3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmLs006W8rCU"
      },
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-taQEGI_8R1_"
      },
      "source": [
        "for i in range(100,1500,100):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , 0, 0, i, 0.2)\n",
        "  if i == 100:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsEpochs.csv') \n",
        "files.download('ResultsEpochs.csv')\n",
        "\n",
        "Results2.to_csv('VarsEpochs.csv') \n",
        "files.download('VarsEpochs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPBLID7EAArS"
      },
      "source": [
        "for i in np.arange(0,0.6,0.1):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , 0, 0, 250, i)\n",
        "  if i == 0:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsDropout.csv') \n",
        "files.download('ResultsDropout.csv')\n",
        "\n",
        "Results2.to_csv('VarsDropout.csv') \n",
        "files.download('VarsDropout.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNmQJQ6bSHf"
      },
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbu3EZPmATIL"
      },
      "source": [
        "for i in np.arange(0,0.6,0.1):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , 64, 0, 250, i)\n",
        "  if i == 0:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsDropout2.csv') \n",
        "files.download('ResultsDropout2.csv')\n",
        "\n",
        "Results2.to_csv('VarsDropout2.csv') \n",
        "files.download('VarsDropout2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYiEcSw8j-0o"
      },
      "source": [
        "for i in np.arange(0,0.6,0.1):\n",
        "  Window_tests, var = LSTM_test(9, 32, 32 , 64, 0, 250, i)\n",
        "  if i == 0:\n",
        "    Results = Window_tests\n",
        "    Results2 = var\n",
        "  else:\n",
        "    Results = pd.concat((Results,Window_tests), axis = 0)\n",
        "    Results2 = pd.concat((Results2,var), axis = 0)\n",
        "\n",
        "print(Results)\n",
        "print(Results2)\n",
        "\n",
        "from google.colab import files\n",
        "Results.to_csv('ResultsDropout3.csv') \n",
        "files.download('ResultsDropout3.csv')\n",
        "\n",
        "Results2.to_csv('VarsDropout3.csv') \n",
        "files.download('VarsDropout3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}