{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA_ANN_Classification_Best_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKX785rK1goa"
      },
      "source": [
        "%tensorflow_version 2.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA_FEoBwu2Cl"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JViL7Zxyvg4b"
      },
      "source": [
        "# Generic\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy import signal\n",
        "from scipy.fft import fftshift\n",
        "import scipy\n",
        "from datetime import datetime\n",
        "\n",
        "#ML Libs\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import scipy.ndimage as nd\n",
        "from keras.models import Sequential,load_model\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras import metrics\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Convolution2D, MaxPooling2D, BatchNormalization, ReLU, LeakyReLU, Conv2D\n",
        "from keras import layers\n",
        "from keras.optimizers import adam_v2, adagrad_v2, rmsprop_v2\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, auc, roc_curve, precision_recall_fscore_support\n",
        "import keras\n",
        "#from keras.layers.experimental import preprocessing\n",
        "\n",
        "\n",
        "Terminal = False\n",
        "\n",
        "#Loading SQI Matched with clinitcal to the Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlEsJsg0vwO9"
      },
      "source": [
        "#Loading SQI Matched with clinical to the Dataframe\n",
        "\n",
        "id = '1DDf_vluJE-zg--41A0zpP5_9-FU-wsJp' # The shareable link id\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Complete_SQIs_with_Clinical.csv')  \n",
        "SQI_C = pd.read_csv('Complete_SQIs_with_Clinical.csv')\n",
        "\n",
        "\n",
        "id = '1zEpiX0yXT16cz3znpo8wjoV_U_8vSoyb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Raw_signals.csv')  \n",
        "Raw = pd.read_csv('Raw_signals.csv')\n",
        "\n",
        "if Terminal:\n",
        "    print(\"\\n SQI with Clinical Match:\")\n",
        "    print(SQI_C)\n",
        "    print(\"\\n Raw Signals\")\n",
        "    print(Raw)\n",
        "\n",
        "\n",
        "#Not really need, here for completeness\n",
        "event = ['event_shock', 'reshock24','diagnosis_admission',\\\n",
        "     'ascites', 'respiratory_distress', 'ventilation_cannula', \\\n",
        "     'ventilation_mechanical', 'ventilation_ncpap', 'bleeding_severe', \\\n",
        "     'cns_abnormal', 'liver_mild', 'pleural_effusion', 'skidney']\n",
        "\n",
        "event_shock = 'shock_admission'\n",
        "\n",
        "SQI_C['keep'] = False\n",
        "for i in range(len(event)):\n",
        "    event_s = event[i]\n",
        "    SQI_C['keep'][SQI_C[event_s] == True] = True\n",
        "    #print(\"\\n Total \", event[i], \" events:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJKtiZxQvxXU"
      },
      "source": [
        "#Mixed, Admitted vs Not, Pre vs Post\n",
        "#Patient 1 pre shock => 2 Hours Each\n",
        "#Patient 2 post shock, in this case P1 and P2 are the same\n",
        "P1times = datetime.strptime('1809202015450000000', '%d%m%Y%H%M%S%f')##########\n",
        "P1timef = datetime.strptime('1809202017450000000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P2times = datetime.strptime('18092020181500000', '%d%m%Y%H%M%S%f')##########\n",
        "P2timef = datetime.strptime('18092020201500000', '%d%m%Y%H%M%S%f')\n",
        "\n",
        "#Patient 3 had no shock\n",
        "#Patient 4 had shock \n",
        "P3times = datetime.strptime('20072020143000000', '%d%m%Y%H%M%S%f')\n",
        "P3timef = datetime.strptime('20072020163000000', '%d%m%Y%H%M%S%f')\n",
        "\n",
        "P4times = datetime.strptime('03112020103000000', '%d%m%Y%H%M%S%f')\n",
        "P4timef = datetime.strptime('03112020123000020', '%d%m%Y%H%M%S%f') #added 20 miliseconds to match the length of Patient 1\n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "Patient1 = Raw[(Raw.study_no == '003-2104') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2104') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "Patient3 = Raw[(Raw.study_no == '003-2162') & (P3times <= Raw.PPG_Datetime) & (P3timef > Raw.PPG_Datetime)]\n",
        "Patient4 = Raw[(Raw.study_no == '003-2028') & (P4times <= Raw.PPG_Datetime) & (P4timef > Raw.PPG_Datetime)]\n",
        "#Patient3 = Raw[(Raw.study_no == '003-2104') & (P3times <= Raw.PPG_Datetime) & (P3timef > Raw.PPG_Datetime)]#############\n",
        "\n",
        "\n",
        "# Patient1['Label'] = 0 \n",
        "# Patient2['Label'] = 1\n",
        "# Patient3['Label'] = 0 \n",
        "# Patient4['Label'] = 1\n",
        "#Patient3['Label'] = 1\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "  print(Patient3)\n",
        "  print(Patient4)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 100)\n",
        "Patient2_w = np.array_split(Patient2, 100)\n",
        "Patient3_w = np.array_split(Patient3, 100)\n",
        "Patient4_w = np.array_split(Patient4, 100)\n",
        "#Patient3_w = np.array_split(Patient3, 50)##########\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),7200,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'PLETH_bpf')#Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'PLETH_bpf')\n",
        "Patient3_w_arr = choose_signal(Patient3_w, 'PLETH_bpf') \n",
        "Patient4_w_arr = choose_signal(Patient4_w, 'PLETH_bpf')\n",
        "#Patient3_w_arr = choose_signal(Patient3_w, 'IR_ADC_bpf')#############\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((100,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((100,1), dtype=int)\n",
        "Patient3_w_arr_l = np.full((100,1),2, dtype=int)\n",
        "Patient4_w_arr_l = np.full((100,1),3, dtype=int)\n",
        "#Patient3_w_arr_l = np.ones((100,1), dtype=int)############\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "\n",
        "Whole_Array_Test = np.empty((30,7))\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "Recall = np.empty((5,1))\n",
        "Precision = np.empty((5,1))\n",
        "F1S = np.empty((5,1))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr,Patient2_w_arr,Patient3_w_arr,Patient4_w_arr))\n",
        "Labels = np.vstack((Patient1_w_arr_l,Patient2_w_arr_l,Patient3_w_arr_l,Patient4_w_arr_l))\n",
        "\n",
        "for i in range(10):\n",
        "#counter2 = 0\n",
        "#for jj in range(20,300,20):\n",
        "  counter = 0 \n",
        "  k = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "  for train_index, test_index in k.split(Data,Labels):\n",
        "    print(\"Fold Number =================================================== \", counter)\n",
        "    X_train = Data[train_index]\n",
        "    X_test = Data[test_index]\n",
        "    y_train = Labels[train_index]\n",
        "    y_test = Labels[test_index]\n",
        "\n",
        "    if Terminal:\n",
        "      print(train_index)\n",
        "      print(test_index)\n",
        "    #Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "    #Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "    Globalmean = np.mean(X_train)\n",
        "    Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "    P_train = (X_train - Globalmean) / Globalstd\n",
        "    P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "    #Empty arrays\n",
        "    #P1Sxx = np.empty((len(P1_training),26,32))\n",
        "    #P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "    PSxx = np.empty((len(P_train),23,32))\n",
        "    PSxxtest = np.empty((len(P_test),23,32))\n",
        "\n",
        "\n",
        "    #Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "    #Patient 1\n",
        "\n",
        "    def spectrogram_arr(Input, Output_arr, fs):\n",
        "      for i in range(len(Input)):\n",
        "        f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "        fmin = 1 # Hz\n",
        "        fmax = 10 # Hz\n",
        "        freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "        # keep only frequencies of interest\n",
        "        f   = f[freq_slice]\n",
        "        Sxx = Sxx[freq_slice,:][0]\n",
        "        Output_arr[i] = Sxx\n",
        "      return Output_arr\n",
        "\n",
        "    X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "    X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "    Terminal = False\n",
        "\n",
        "    if Terminal:\n",
        "      print(X_train.shape)\n",
        "      print(y_train.shape)\n",
        "      print(X_test.shape)\n",
        "      print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "    #tf.random.set_seed(1234)\n",
        "\n",
        "    #converting to categorical\n",
        "    y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "    y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "    #X_trainf = X_train.reshape(-1, 26*32)\n",
        "    #X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "    #print(X_train)\n",
        "    print(X_test.shape)\n",
        "    pca = PCA(n_components=60) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "    X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "    pca = pca.fit(X_train_2d)\n",
        "    X_trainf = pca.transform(X_train_2d)\n",
        "    #pca = PCA(n_components=15)\n",
        "    #pca = pca.fit(X_test_2d)\n",
        "    X_testf = pca.transform(X_test_2d)\n",
        "    # for i in range(X_train.shape[0]):\n",
        "    #   pca = pca.fit(X_train[i,:,:])\n",
        "    #   X_trainf = pca.transform(X_train[i,:,:])\n",
        "    # for i in range(X_test.shape[0]):\n",
        "    #   pca = pca.fit(X_test[i,:,:])\n",
        "    #   X_testf = pca.transform(X_test[i,:,:])\n",
        "\n",
        "    Terminal = True\n",
        "    \n",
        "    print(X_trainf.shape)\n",
        "    print(X_testf.shape)\n",
        "    \n",
        "    if Terminal:\n",
        "      print('Train Labels')\n",
        "      print(y_trainf.shape)\n",
        "      print('Train Data')\n",
        "      print(X_trainf.shape)\n",
        "\n",
        "    # Simple ANN Start ========================================================================================\n",
        "\n",
        "    # model = Sequential()\n",
        "    # model.add(Dense(16,'relu', input_shape=(15,)))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    # #model.add(BatchNormalization())\n",
        "    # #model.add(Dense(32,'relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(Dense(64,'relu'))\n",
        "    # model.add(Dropout(0.4))\n",
        "    # model.add(Dense(16,'relu'))\n",
        "    # model.add(Dropout(0.4))\n",
        "    # model.add(Dense(4))\n",
        "    # model.add(Activation('Softmax'))\n",
        "\n",
        "    from keras.regularizers import l1_l2\n",
        "    # model = Sequential()\n",
        "    # model.add(Dense(64,'relu', input_shape=(60,), kernel_initializer='glorot_uniform'))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    # #model.add(BatchNormalization())\n",
        "    # model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    # model.add(Dropout(0.4))\n",
        "    # model.add(Dense(256,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    # model.add(Dropout(0.3))\n",
        "    # model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))\n",
        "    # model.add(Dropout(0.3))\n",
        "    # model.add(Dense(64,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))\n",
        "    # model.add(Dropout(0.4))\n",
        "    # model.add(Dense(64,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))\n",
        "    # #model.add(Dropout(0.2))\n",
        "    # model.add(Dense(4))\n",
        "    # model.add(Activation('Softmax'))\n",
        "\n",
        "    # model = Sequential()\n",
        "    # model.add(Dense(32,'relu', input_shape=(60,), kernel_initializer='glorot_uniform'))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    # model.add(Dropout(0.6))\n",
        "    # model.add(Dense(512,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    # model.add(Dense(512,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    # model.add(Dropout(0.6))\n",
        "    # model.add(Dense(256,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    # model.add(Dense(4))\n",
        "    # model.add(Activation('Softmax'))\n",
        "\n",
        "    # # . . . \n",
        "    # #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    # model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    # history = model.fit(X_trainf, y_trainf, epochs=150, batch_size=32, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64,'relu', input_shape=(60,), kernel_initializer='glorot_uniform'))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(64,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(4))\n",
        "    model.add(Activation('Softmax'))\n",
        "\n",
        "    # . . . \n",
        "    #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    model.compile(optimizer=adam_v2.Adam(learning_rate=3e-3),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    history = model.fit(X_trainf, y_trainf, epochs=150, batch_size=32, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "    # ANN ==========================================================================================\n",
        "\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Accuracy and Loss')\n",
        "    plt.ylabel('accuracy/loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "    y_prob= model.predict(X_testf)\n",
        "    y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "    PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "    print('\\nConfusion Matrix on Unseen Data:')\n",
        "    print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "    print('\\nReport on Unseen Data:')\n",
        "    print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "    #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "    # probs = model.predict_proba(X_testf)\n",
        "    # preds = probs[:,1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    Score_array[counter] = score[1]\n",
        "    Recall[counter] = PRF[1]\n",
        "    Precision[counter] = PRF[0] \n",
        "    F1S[counter] =  PRF[2]\n",
        "    counter = counter + 1\n",
        "\n",
        "\n",
        "\n",
        "  print(Score_array)\n",
        "\n",
        "  print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "  print(np.max(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "  print(np.min(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "  print(np.sum(Score_array)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "  print(np.sum(Recall)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "  print(np.sum(Precision)/5)\n",
        "  print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "  print(np.sum(F1S)/5)\n",
        "\n",
        "  # Whole_Array_Test[counter2][0]= jj\n",
        "  # Whole_Array_Test[counter2][1]= np.max(Score_array)\n",
        "  # Whole_Array_Test[counter2][2]= np.min(Score_array)\n",
        "  # Whole_Array_Test[counter2][3]= np.sum(Score_array)/5\n",
        "  # Whole_Array_Test[counter2][4]= np.sum(Recall)/5\n",
        "  # Whole_Array_Test[counter2][5]= np.sum(Precision)/5\n",
        "  # Whole_Array_Test[counter2][6]= np.sum(F1S)/5\n",
        "    \n",
        "    \n",
        "  #   print(Whole_Array_Test[counter2])\n",
        "      \n",
        "  #   counter2 = counter2 + 1\n",
        "\n",
        "  # print(Whole_Array_Test)\n",
        "  # Results = pd.DataFrame(Whole_Array_Test)\n",
        "  # Results.to_csv('Model1PCA.csv') \n",
        "  # files.download('Model1PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UyQGLI-6DsJ"
      },
      "source": [
        "from google.colab import files\n",
        "Results = pd.DataFrame(Whole_Array_Test)\n",
        "Results.to_csv('Model2PCA.csv') \n",
        "files.download('Model2PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7LIhyn_3AV"
      },
      "source": [
        "#SHOCK ADMISSION VS SHOCK NOT ADMISSION  # 2 Hour Samples Each\n",
        "#Patient 1 had no shock\n",
        "#Patient 2 had shock \n",
        "P1times = datetime.strptime('20072020143000000', '%d%m%Y%H%M%S%f')\n",
        "P1timef = datetime.strptime('20072020163000000', '%d%m%Y%H%M%S%f')\n",
        "\n",
        "P2times = datetime.strptime('03112020103000000', '%d%m%Y%H%M%S%f')\n",
        "P2timef = datetime.strptime('03112020123000020', '%d%m%Y%H%M%S%f') #added 20 miliseconds to match the length of Patient 1\n",
        "\n",
        "#P3times = datetime.strptime('18092020203000000', '%d%m%Y%H%M%S%f')##########\n",
        "#P3timef = datetime.strptime('18092020213000000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "Patient1 = Raw[(Raw.study_no == '003-2162') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2028') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "#Patient3 = Raw[(Raw.study_no == '003-2104') & (P3times <= Raw.PPG_Datetime) & (P3timef > Raw.PPG_Datetime)]#############\n",
        "\n",
        "\n",
        "Patient1['Label'] = 0 \n",
        "Patient2['Label'] = 1\n",
        "#Patient3['Label'] = 1\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "  #print(Patient3)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 100)\n",
        "Patient2_w = np.array_split(Patient2, 100)\n",
        "#Patient3_w = np.array_split(Patient3, 50)##########\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),7200,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'PLETH_bpf') #Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'PLETH_bpf')\n",
        "#Patient3_w_arr = choose_signal(Patient3_w, 'IR_ADC_bpf')#############\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((100,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((100,1), dtype=int)\n",
        "#Patient3_w_arr_l = np.ones((100,1), dtype=int)############\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "\n",
        "\n",
        "# Whole_Array_Test = np.empty((30,7))\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "Recall = np.empty((5,1))\n",
        "Precision = np.empty((5,1))\n",
        "F1S = np.empty((5,1))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr,Patient2_w_arr))\n",
        "Labels = np.vstack((Patient1_w_arr_l,Patient2_w_arr_l))\n",
        "\n",
        "\n",
        "# counter2 = 0\n",
        "# for jj in range(20,180,20):\n",
        "for i in range(10):\n",
        "  counter = 0\n",
        "  k = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "  for train_index, test_index in k.split(Data,Labels):\n",
        "    print(\"Fold Number =================================================== \", counter)\n",
        "    X_train = Data[train_index]\n",
        "    X_test = Data[test_index]\n",
        "    y_train = Labels[train_index]\n",
        "    y_test = Labels[test_index]\n",
        "\n",
        "    if Terminal:\n",
        "      print(train_index)\n",
        "      print(test_index)\n",
        "    #Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "    #Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "    Globalmean = np.mean(X_train)\n",
        "    Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "    P_train = (X_train - Globalmean) / Globalstd\n",
        "    P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "    #Empty arrays\n",
        "    #P1Sxx = np.empty((len(P1_training),26,32))\n",
        "    #P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "    PSxx = np.empty((len(P_train),26,32))\n",
        "    PSxxtest = np.empty((len(P_test),26,32))\n",
        "\n",
        "\n",
        "    #Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "    #Patient 1\n",
        "\n",
        "    def spectrogram_arr(Input, Output_arr, fs):\n",
        "      for i in range(len(Input)):\n",
        "        f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "        fmin = 0 # Hz\n",
        "        fmax = 10 # Hz\n",
        "        freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "        # keep only frequencies of interest\n",
        "        f   = f[freq_slice]\n",
        "        Sxx = Sxx[freq_slice,:][0]\n",
        "        Output_arr[i] = Sxx\n",
        "      return Output_arr\n",
        "\n",
        "    X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "    X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "    Terminal = False\n",
        "\n",
        "    if Terminal:\n",
        "      print(X_train.shape)\n",
        "      print(y_train.shape)\n",
        "      print(X_test.shape)\n",
        "      print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "    #tf.random.set_seed(1234)\n",
        "\n",
        "    #converting to categorical\n",
        "    y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "    y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "    #X_trainf = X_train.reshape(-1, 26*32)\n",
        "    #X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "    # #print(X_train)\n",
        "    # print(X_test.shape)\n",
        "    # pca = PCA(n_components=17) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    # for i in range(X_train.shape[2]):\n",
        "    #   pca = pca.fit(X_train[:,:,i])\n",
        "    #   X_trainf = pca.transform(X_train[:,:,i])\n",
        "    #   X_testf = pca.transform(X_test[:,:,i])\n",
        "\n",
        "    pca = PCA(n_components=120) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "    X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "    pca = pca.fit(X_train_2d)\n",
        "    X_trainf = pca.transform(X_train_2d)\n",
        "    X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "    Terminal = True\n",
        "    print(X_trainf.shape)\n",
        "    print(X_testf.shape)\n",
        "    \n",
        "    if Terminal:\n",
        "      print('Train Labels')\n",
        "      print(y_trainf.shape)\n",
        "      print('Train Data')\n",
        "      print(X_train.shape)\n",
        "\n",
        "    # Simple ANN Start ========================================================================================\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64,'relu', input_shape=(120,), kernel_initializer='glorot_uniform'))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(128,'relu', bias_regularizer=l2(0.001), kernel_initializer='glorot_uniform'))#, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('Softmax'))\n",
        "\n",
        "    # . . . \n",
        "    #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    history = model.fit(X_trainf, y_trainf, epochs=200, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "    # #500 epochs for IR_ADC\n",
        "\n",
        "    # ANN ==========================================================================================\n",
        "\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Accuracy and Loss')\n",
        "    plt.ylabel('accuracy/loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "    y_prob= model.predict(X_testf)\n",
        "    y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "    PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "    print('\\nConfusion Matrix on Unseen Data:')\n",
        "    print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "    print('\\nReport on Unseen Data:')\n",
        "    print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "    #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "    # probs = model.predict_proba(X_testf)\n",
        "    # preds = probs[:,1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "    Score_array[counter] = score[1]\n",
        "    Recall[counter] = PRF[1]\n",
        "    Precision[counter] = PRF[0] \n",
        "    F1S[counter] =  PRF[2]\n",
        "    counter = counter + 1\n",
        "\n",
        "  print(Score_array)\n",
        "\n",
        "  print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "  print(np.max(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "  print(np.min(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "  print(np.sum(Score_array)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "  print(np.sum(Recall)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "  print(np.sum(Precision)/5)\n",
        "  print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "  print(np.sum(F1S)/5)\n",
        "\n",
        "  #   Whole_Array_Test[counter2][0]= jj\n",
        "  #   Whole_Array_Test[counter2][1]= np.max(Score_array)\n",
        "  #   Whole_Array_Test[counter2][2]= np.min(Score_array)\n",
        "  #   Whole_Array_Test[counter2][3]= np.sum(Score_array)/5\n",
        "  #   Whole_Array_Test[counter2][4]= np.sum(Recall)/5\n",
        "  #   Whole_Array_Test[counter2][5]= np.sum(Precision)/5\n",
        "  #   Whole_Array_Test[counter2][6]= np.sum(F1S)/5\n",
        "\n",
        "  #   print(Whole_Array_Test[counter2])\n",
        "      \n",
        "  #   counter2 = counter2 + 1\n",
        "\n",
        "  # print(Whole_Array_Test)\n",
        "  # Results = pd.DataFrame(Whole_Array_Test)\n",
        "  # Results.to_csv('Model1PCA.csv') \n",
        "  # files.download('Model1PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5br0vj_F9vL"
      },
      "source": [
        "#SHOCK  VS NO SHOCK without Shock In Admission # 2 Hour Samples Each\n",
        "#Patient 1 pre shock\n",
        "#Patient 2 post shock, in this case P1 and P2 are the same\n",
        "P1times = datetime.strptime('1809202015450000000', '%d%m%Y%H%M%S%f')##########\n",
        "P1timef = datetime.strptime('1809202017450000000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P2times = datetime.strptime('18092020181500000', '%d%m%Y%H%M%S%f')##########\n",
        "P2timef = datetime.strptime('18092020201500000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "Patient1 = Raw[(Raw.study_no == '003-2104') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2104') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "\n",
        "\n",
        "Patient1['Label'] = 0 \n",
        "Patient2['Label'] = 1\n",
        "#Patient3['Label'] = 1\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "  #print(Patient3)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 100)\n",
        "Patient2_w = np.array_split(Patient2, 100)\n",
        "#Patient3_w = np.array_split(Patient3, 50)##########\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),7200,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'PLETH_bpf') #Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'PLETH_bpf')\n",
        "#Patient3_w_arr = choose_signal(Patient3_w, 'IR_ADC_bpf')#############\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((100,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((100,1), dtype=int)\n",
        "#Patient3_w_arr_l = np.ones((100,1), dtype=int)############\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "Whole_Array_Test = np.empty((30,7))\n",
        "Score_array = np.empty((5,1))\n",
        "Recall = np.empty((5,1))\n",
        "Precision = np.empty((5,1))\n",
        "F1S = np.empty((5,1))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr,Patient2_w_arr))\n",
        "Labels = np.vstack((Patient1_w_arr_l,Patient2_w_arr_l))\n",
        "\n",
        "\n",
        "counter2 = 0\n",
        "for jj in range(20,180,20):\n",
        "  counter = 0\n",
        "  k = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "  for train_index, test_index in k.split(Data,Labels):\n",
        "    print(\"Fold Number =================================================== \", counter)\n",
        "    X_train = Data[train_index]\n",
        "    X_test = Data[test_index]\n",
        "    y_train = Labels[train_index]\n",
        "    y_test = Labels[test_index]\n",
        "\n",
        "    if Terminal:\n",
        "      print(train_index)\n",
        "      print(test_index)\n",
        "    #Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "    #Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "    Globalmean = np.mean(X_train)\n",
        "    Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "    P_train = (X_train - Globalmean) / Globalstd\n",
        "    P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "    #Empty arrays\n",
        "    #P1Sxx = np.empty((len(P1_training),26,32))\n",
        "    #P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "    PSxx = np.empty((len(P_train),26,32))\n",
        "    PSxxtest = np.empty((len(P_test),26,32))\n",
        "\n",
        "\n",
        "    #Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "    #Patient 1\n",
        "\n",
        "    def spectrogram_arr(Input, Output_arr, fs):\n",
        "      for i in range(len(Input)):\n",
        "        f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "        fmin = 0 # Hz\n",
        "        fmax = 10 # Hz\n",
        "        freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "        # keep only frequencies of interest\n",
        "        f   = f[freq_slice]\n",
        "        Sxx = Sxx[freq_slice,:][0]\n",
        "        Output_arr[i] = Sxx\n",
        "      return Output_arr\n",
        "\n",
        "    X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "    X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "    Terminal = False\n",
        "\n",
        "    if Terminal:\n",
        "      print(X_train.shape)\n",
        "      print(y_train.shape)\n",
        "      print(X_test.shape)\n",
        "      print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "    #tf.random.set_seed(1234)\n",
        "\n",
        "    #converting to categorical\n",
        "    y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "    y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "    #X_trainf = X_train.reshape(-1, 26*32)\n",
        "    #X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "    # #print(X_train)\n",
        "    # print(X_test.shape)\n",
        "    # pca = PCA(n_components=23) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    # for i in range(X_train.shape[2]):\n",
        "    #   pca = pca.fit(X_train[:,:,i])\n",
        "    #   X_trainf = pca.transform(X_train[:,:,i])\n",
        "    #   X_testf = pca.transform(X_test[:,:,i])\n",
        "\n",
        "    pca = PCA(n_components=jj) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "    X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "    pca = pca.fit(X_train_2d)\n",
        "    X_trainf = pca.transform(X_train_2d)\n",
        "    X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "    Terminal = True\n",
        "    print(X_trainf.shape)\n",
        "    print(X_testf.shape)\n",
        "    \n",
        "    if Terminal:\n",
        "      print('Train Labels')\n",
        "      print(y_trainf.shape)\n",
        "      print('Train Data')\n",
        "      print(X_train.shape)\n",
        "\n",
        "    # Simple ANN Start ========================================================================================\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128,'relu', input_shape=(jj,)))#,use_bias=True, kernel_initializer='glorot_uniform'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(256,'relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(64,'relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('Softmax'))\n",
        "\n",
        "    # . . . \n",
        "    #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    history = model.fit(X_trainf, y_trainf, epochs=150, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "    # ANN ==========================================================================================\n",
        "\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Accuracy and Loss')\n",
        "    plt.ylabel('accuracy/loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "    y_prob= model.predict(X_testf)\n",
        "    y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "    PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "    print('\\nConfusion Matrix on Unseen Data:')\n",
        "    print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "    print('\\nReport on Unseen Data:')\n",
        "    print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "    #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "    # probs = model.predict_proba(X_testf)\n",
        "    # preds = probs[:,1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "    Score_array[counter] = score[1]\n",
        "    Recall[counter] = PRF[1]\n",
        "    Precision[counter] = PRF[0] \n",
        "    F1S[counter] =  PRF[2]\n",
        "    counter = counter + 1\n",
        "\n",
        "  print(Score_array)\n",
        "\n",
        "  print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "  print(np.max(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "  print(np.min(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "  print(np.sum(Score_array)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "  print(np.sum(Recall)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "  print(np.sum(Precision)/5)\n",
        "  print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "  print(np.sum(F1S)/5)\n",
        "\n",
        "  Whole_Array_Test[counter2][0]= jj\n",
        "  Whole_Array_Test[counter2][1]= np.max(Score_array)\n",
        "  Whole_Array_Test[counter2][2]= np.min(Score_array)\n",
        "  Whole_Array_Test[counter2][3]= np.sum(Score_array)/5\n",
        "  Whole_Array_Test[counter2][4]= np.sum(Recall)/5\n",
        "  Whole_Array_Test[counter2][5]= np.sum(Precision)/5\n",
        "  Whole_Array_Test[counter2][6]= np.sum(F1S)/5\n",
        "\n",
        "  print(Whole_Array_Test[counter2])\n",
        "    \n",
        "  counter2 = counter2 + 1\n",
        "\n",
        "print(Whole_Array_Test)\n",
        "Results = pd.DataFrame(Whole_Array_Test)\n",
        "Results.to_csv('Model3PCA.csv') \n",
        "files.download('Model3PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrHC_vjqGijk"
      },
      "source": [
        "#Patient 1 pre shock (Admitted with Shock)\n",
        "#Patient 2 post shock, in this case P1 and P2 are the same 2009\n",
        "#Unfortunately Post shock data is much shorter and therefore we split it into smaller windows (Data is still less)\n",
        "Terminal = False\n",
        "if Terminal:\n",
        "  #Print the Study\n",
        "  print('Study 003-2009 Datetime:')\n",
        "  print(Raw[(Raw.study_no == '003-2009')].PPG_Datetime)\n",
        "\n",
        "  #Printing Time of Event\n",
        "  print('PPG Shock Event Start -2009:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2009') & (SQI_C.keep == True)].PPG_w_s)\n",
        "  print('PPG Shock Event End -2009:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2009') & (SQI_C.keep == True)].PPG_w_f)\n",
        "\n",
        "  #Print the Study\n",
        "  print('Study 003-2103 Datetime:')\n",
        "  print(Raw[(Raw.study_no == '003-2103')].PPG_Datetime)\n",
        "\n",
        "  #Printing Time of Event\n",
        "  print('PPG Shock Event Start -2103:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2103') & (SQI_C.keep == True)].PPG_w_s)\n",
        "  print('PPG Shock Event End -2103:')\n",
        "  print(SQI_C[(SQI_C.study_no == '003-2103') & (SQI_C.keep == True)].PPG_w_f)\n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "P1times = datetime.strptime('28072020160400000', '%d%m%Y%H%M%S%f')########## Start Until Shock\n",
        "P1timef = datetime.strptime('29072020032820200', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P2times = datetime.strptime('29072020033146280', '%d%m%Y%H%M%S%f')########## Shock Until the End\n",
        "P2timef = datetime.strptime('29072020044900000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "# P3times = datetime.strptime('29072020033146280', '%d%m%Y%H%M%S%f')########## Shock Until the End for Patient 2103\n",
        "# P3timef = datetime.strptime('18092020044800000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "#Extracting\n",
        "Patient1 = Raw[(Raw.study_no == '003-2009') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2009') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "#Patient3 = Raw[(Raw.study_no == '003-2103') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "\n",
        "\n",
        "#Unnecessary Label Added\n",
        "Patient1['Label'] = 0 \n",
        "Patient2['Label'] = 1\n",
        "#Patient3['Label'] = 1\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 684) #Split into 60 Sec Windows\n",
        "Patient2_w = np.array_split(Patient2, 77)\n",
        "#Patient3_w = np.array_split(Patient3, 50)##########\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),6000,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'IR_ADC_bpf') #Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'IR_ADC_bpf')\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((684,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((77,1), dtype=int)\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "Whole_Array_Test = np.empty((30,7))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr[-77:,:],Patient2_w_arr)) #for a balanced model we use slicing \n",
        "Labels = np.vstack((Patient1_w_arr_l[-77:,:],Patient2_w_arr_l))\n",
        "\n",
        "Recall = np.empty((5,1))\n",
        "Precision = np.empty((5,1))\n",
        "F1S = np.empty((5,1))\n",
        "\n",
        "\n",
        "counter2 = 0\n",
        "for jj in range(80,81,80):\n",
        "  counter = 0\n",
        "  k = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "  for train_index, test_index in k.split(Data,Labels):\n",
        "    print(\"Fold Number =================================================== \", counter)\n",
        "    X_train = Data[train_index]\n",
        "    X_test = Data[test_index]\n",
        "    y_train = Labels[train_index]\n",
        "    y_test = Labels[test_index]\n",
        "\n",
        "    if Terminal:\n",
        "      print(train_index)\n",
        "      print(test_index)\n",
        "    #Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "    #Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "    Globalmean = np.mean(X_train)\n",
        "    Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "    P_train = (X_train - Globalmean) / Globalstd\n",
        "    P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "    #Empty arrays\n",
        "    #P1Sxx = np.empty((len(P1_training),26,32))\n",
        "    #P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "    PSxx = np.empty((len(P_train),26,26))\n",
        "    PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "\n",
        "    #Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "    #Patient 1\n",
        "\n",
        "    def spectrogram_arr(Input, Output_arr, fs):\n",
        "      for i in range(len(Input)):\n",
        "        f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "        fmin = 0 # Hz\n",
        "        fmax = 10 # Hz\n",
        "        freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "        # keep only frequencies of interest\n",
        "        f   = f[freq_slice]\n",
        "        Sxx = Sxx[freq_slice,:][0]\n",
        "        Output_arr[i] = Sxx\n",
        "      return Output_arr\n",
        "\n",
        "    X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "    X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "    Terminal = False\n",
        "\n",
        "    if Terminal:\n",
        "      print(X_train.shape)\n",
        "      print(y_train.shape)\n",
        "      print(X_test.shape)\n",
        "      print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "    #tf.random.set_seed(1234)\n",
        "\n",
        "    #converting to categorical\n",
        "    y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "    y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "    #X_trainf = X_train.reshape(-1, 26*32)\n",
        "    #X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "    # #print(X_train)\n",
        "    # print(X_test.shape)\n",
        "    # pca = PCA(n_components=26) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    # for i in range(X_train.shape[2]):\n",
        "    #   pca = pca.fit(X_train[:,:,i])\n",
        "    #   X_trainf = pca.transform(X_train[:,:,i])\n",
        "    #   X_testf = pca.transform(X_test[:,:,i])\n",
        "\n",
        "    pca = PCA(n_components=jj) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "    X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "    pca = pca.fit(X_train_2d)\n",
        "    X_trainf = pca.transform(X_train_2d)\n",
        "    X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "    Terminal = True\n",
        "    print(X_trainf.shape)\n",
        "    print(X_testf.shape)\n",
        "    \n",
        "    if Terminal:\n",
        "      print('Train Labels')\n",
        "      print(y_trainf.shape)\n",
        "      print('Train Data')\n",
        "      print(X_train.shape)\n",
        "\n",
        "    # Simple ANN Start ========================================================================================\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256,'relu', input_shape=(jj,), kernel_initializer ='glorot_uniform' ))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(512,'relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(256,'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('Softmax'))\n",
        "\n",
        "    # . . . \n",
        "    #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    model.summary()\n",
        "    history = model.fit(X_trainf, y_trainf, epochs=600, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "    # ANN ==========================================================================================\n",
        "\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Accuracy and Loss')\n",
        "    plt.ylabel('accuracy/loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "    y_prob= model.predict(X_testf)\n",
        "    y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "    PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "    print('\\nConfusion Matrix on Unseen Data:')\n",
        "    print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "    print('\\nReport on Unseen Data:')\n",
        "    print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "    #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "    # probs = model.predict_proba(X_testf)\n",
        "    # preds = probs[:,1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "    Score_array[counter] = score[1]\n",
        "    Recall[counter] = PRF[1]\n",
        "    Precision[counter] = PRF[0] \n",
        "    F1S[counter] =  PRF[2]\n",
        "    counter = counter + 1\n",
        "\n",
        "  print(Score_array)\n",
        "\n",
        "  print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "  print(np.max(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "  print(np.min(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "  print(np.sum(Score_array)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "  print(np.sum(Recall)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "  print(np.sum(Precision)/5)\n",
        "  print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "  print(np.sum(F1S)/5)\n",
        "\n",
        "  Whole_Array_Test[counter2][0]= jj\n",
        "  Whole_Array_Test[counter2][1]= np.max(Score_array)\n",
        "  Whole_Array_Test[counter2][2]= np.min(Score_array)\n",
        "  Whole_Array_Test[counter2][3]= np.sum(Score_array)/5\n",
        "  Whole_Array_Test[counter2][4]= np.sum(Recall)/5\n",
        "  Whole_Array_Test[counter2][5]= np.sum(Precision)/5\n",
        "  Whole_Array_Test[counter2][6]= np.sum(F1S)/5\n",
        "\n",
        "  print(Whole_Array_Test[counter2])\n",
        "    \n",
        "  counter2 = counter2 + 1\n",
        "\n",
        "print(Whole_Array_Test)\n",
        "#Results = pd.DataFrame(Whole_Array_Test)\n",
        "#Results.to_csv('Model4PCA.csv') \n",
        "#files.download('Model4PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugI5bcxxKwTC"
      },
      "source": [
        "#Mixed, Admitted vs Not, Pre vs Post\n",
        "#Patient 1 pre shock => 2 Hours Each\n",
        "#Patient 2 post shock, in this case P1 and P2 are the same\n",
        "P1times = datetime.strptime('1809202015450000000', '%d%m%Y%H%M%S%f')##########\n",
        "P1timef = datetime.strptime('1809202017450000000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P2times = datetime.strptime('18092020181500000', '%d%m%Y%H%M%S%f')##########\n",
        "P2timef = datetime.strptime('18092020201500000', '%d%m%Y%H%M%S%f')\n",
        "\n",
        "#Patient 3 had no shock\n",
        "#Patient 4 had shock \n",
        "P3times = datetime.strptime('20072020143000000', '%d%m%Y%H%M%S%f')\n",
        "P3timef = datetime.strptime('20072020163000000', '%d%m%Y%H%M%S%f')\n",
        "\n",
        "P4times = datetime.strptime('03112020103000000', '%d%m%Y%H%M%S%f')\n",
        "P4timef = datetime.strptime('03112020123000020', '%d%m%Y%H%M%S%f') #added 20 miliseconds to match the length of Patient 1\n",
        "\n",
        "\n",
        "P5times = datetime.strptime('28072020160400000', '%d%m%Y%H%M%S%f')########## Start Until Shock\n",
        "P5timef = datetime.strptime('29072020032820200', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "P6times = datetime.strptime('29072020033146280', '%d%m%Y%H%M%S%f')########## Shock Until the End\n",
        "P6timef = datetime.strptime('18092020044800000', '%d%m%Y%H%M%S%f') \n",
        "\n",
        "\n",
        "#Extracting\n",
        "\n",
        "\n",
        "Raw.PPG_Datetime = pd.to_datetime(Raw.PPG_Datetime)\n",
        "\n",
        "Patient1 = Raw[(Raw.study_no == '003-2104') & (P1times <= Raw.PPG_Datetime) & (P1timef > Raw.PPG_Datetime)]\n",
        "Patient2 = Raw[(Raw.study_no == '003-2104') & (P2times <= Raw.PPG_Datetime) & (P2timef > Raw.PPG_Datetime)]\n",
        "Patient3 = Raw[(Raw.study_no == '003-2162') & (P3times <= Raw.PPG_Datetime) & (P3timef > Raw.PPG_Datetime)]\n",
        "Patient4 = Raw[(Raw.study_no == '003-2028') & (P4times <= Raw.PPG_Datetime) & (P4timef > Raw.PPG_Datetime)]\n",
        "Patient5 = Raw[(Raw.study_no == '003-2009') & (P5times <= Raw.PPG_Datetime) & (P5timef > Raw.PPG_Datetime)]\n",
        "Patient6 = Raw[(Raw.study_no == '003-2009') & (P6times <= Raw.PPG_Datetime) & (P6timef > Raw.PPG_Datetime)]\n",
        "\n",
        "\n",
        "if Terminal:\n",
        "  print(Patient1)\n",
        "  print(Patient2)\n",
        "  print(Patient3)\n",
        "  print(Patient4)\n",
        "  print(Patient5)\n",
        "  print(Patient6)\n",
        "\n",
        "#Splititng aerray into windows\n",
        "Patient1_w = np.array_split(Patient1, 120)\n",
        "Patient2_w = np.array_split(Patient2, 120)\n",
        "Patient3_w = np.array_split(Patient3, 120)\n",
        "Patient4_w = np.array_split(Patient4, 120)\n",
        "Patient5_w = np.array_split(Patient5, 684)\n",
        "Patient6_w = np.array_split(Patient6, 77)\n",
        "\n",
        "def choose_signal(Patient_w, signal):\n",
        "  Patient_w_arr = np.empty((len(Patient_w),6000,))\n",
        "  for i in range(len(Patient_w)):\n",
        "    Patient_w_arr[i] = Patient_w[i][signal]\n",
        "  \n",
        "  return Patient_w_arr\n",
        "\n",
        "Patient1_w_arr = choose_signal(Patient1_w, 'PLETH_bpf') #Change here for PLETH\n",
        "Patient2_w_arr = choose_signal(Patient2_w, 'PLETH_bpf')\n",
        "Patient3_w_arr = choose_signal(Patient3_w, 'PLETH_bpf') \n",
        "Patient4_w_arr = choose_signal(Patient4_w, 'PLETH_bpf')\n",
        "Patient5_w_arr = choose_signal(Patient5_w, 'PLETH_bpf') \n",
        "Patient6_w_arr = choose_signal(Patient6_w, 'PLETH_bpf')\n",
        "\n",
        "\n",
        "Patient1_w_arr_l = np.zeros((120,1), dtype=int)\n",
        "Patient2_w_arr_l = np.ones((120,1), dtype=int)\n",
        "Patient3_w_arr_l = np.full((120,1),2, dtype=int)\n",
        "Patient4_w_arr_l = np.full((120,1),3, dtype=int)\n",
        "Patient5_w_arr_l = np.full((684,1),4, dtype=int)\n",
        "Patient6_w_arr_l = np.full((77,1),5, dtype=int)\n",
        "\n",
        "#Defining Sampling rate\n",
        "fs = 100\n",
        "\n",
        "Score_array = np.empty((5,1))\n",
        "\n",
        "##K-FOLD ML=====================================================================\n",
        "Data = np.vstack((Patient1_w_arr[-77:,:],Patient2_w_arr[:77,:],Patient3_w_arr[-77:,:],Patient4_w_arr[-77:,:],Patient5_w_arr[-77:,:],Patient6_w_arr))\n",
        "Labels = np.vstack((Patient1_w_arr_l[-77:,:],Patient2_w_arr_l[:77,:],Patient3_w_arr_l[-77:,:],Patient4_w_arr_l[-77:,:],Patient5_w_arr_l[-77:,:],Patient6_w_arr_l))\n",
        "\n",
        "#making the dataset balanced\n",
        "\n",
        "Recall = np.empty((5,1))\n",
        "Precision = np.empty((5,1))\n",
        "F1S = np.empty((5,1))\n",
        "\n",
        "\n",
        "Whole_Array_Test = np.empty((30,7))\n",
        "\n",
        "counter2 = 0\n",
        "for jj in range(80,81,80):\n",
        "  counter = 0\n",
        "  k = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "  for train_index, test_index in k.split(Data,Labels):\n",
        "    print(\"Fold Number =================================================== \", counter)\n",
        "    X_train = Data[train_index]\n",
        "    X_test = Data[test_index]\n",
        "    y_train = Labels[train_index]\n",
        "    y_test = Labels[test_index]\n",
        "\n",
        "    if Terminal:\n",
        "      print(train_index)\n",
        "      print(test_index)\n",
        "    #Extracting mean and std from training se to apply onto both the training and test set to avoid data leakage\n",
        "\n",
        "    #Training_sets = np.vstack((P1_tr,P2_tr))\n",
        "    Globalmean = np.mean(X_train)\n",
        "    Globalstd = np.std(X_train)\n",
        "\n",
        "\n",
        "    P_train = (X_train - Globalmean) / Globalstd\n",
        "    P_test = (X_test - Globalmean) / Globalstd\n",
        "\n",
        "    #Empty arrays\n",
        "    #P1Sxx = np.empty((len(P1_training),26,32))\n",
        "    #P1Sxxtest = np.empty((len(P1_test),26,32))\n",
        "    PSxx = np.empty((len(P_train),26,26))\n",
        "    PSxxtest = np.empty((len(P_test),26,26))\n",
        "\n",
        "\n",
        "    #Calculating spectrograms for each Patient separately on each window (120 windows) within 0 and 20 Hz\n",
        "    #Patient 1\n",
        "\n",
        "    def spectrogram_arr(Input, Output_arr, fs):\n",
        "      for i in range(len(Input)):\n",
        "        f, t, Sxx = signal.spectrogram(Input[i], fs)\n",
        "        fmin = 0 # Hz\n",
        "        fmax = 10 # Hz\n",
        "        freq_slice = np.where((f >= fmin) & (f <= fmax))\n",
        "        # keep only frequencies of interest\n",
        "        f   = f[freq_slice]\n",
        "        Sxx = Sxx[freq_slice,:][0]\n",
        "        Output_arr[i] = Sxx\n",
        "      return Output_arr\n",
        "\n",
        "    X_train = spectrogram_arr(P_train, PSxx, fs)\n",
        "    X_test = spectrogram_arr(P_test, PSxxtest, fs)\n",
        "\n",
        "    Terminal = False\n",
        "\n",
        "    if Terminal:\n",
        "      print(X_train.shape)\n",
        "      print(y_train.shape)\n",
        "      print(X_test.shape)\n",
        "      print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #MACHINE LEARNING MODEL ==================CUSTOM CNN=========================\n",
        "    #tf.random.set_seed(1234)\n",
        "\n",
        "    #converting to categorical\n",
        "    y_trainf = tf.keras.utils.to_categorical(y_train)\n",
        "    y_testf = tf.keras.utils.to_categorical(y_test)\n",
        "    #X_trainf = X_train.reshape(-1, 26*32)\n",
        "    #X_testf = X_test.reshape(-1, 26*32)\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "    # #print(X_train)\n",
        "    # print(X_test.shape)\n",
        "    # pca = PCA(n_components=19) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    # for i in range(X_train.shape[2]):\n",
        "    #   pca = pca.fit(X_train[:,:,i])\n",
        "    #   X_trainf = pca.transform(X_train[:,:,i])\n",
        "    #   X_testf = pca.transform(X_test[:,:,i])\n",
        "\n",
        "    pca = PCA(n_components=jj) #No. OF COMPONENTS CHOSEN EMPIRICALLY\n",
        "    X_train_2d = np.array([features_2d.flatten() for features_2d in X_train])\n",
        "    X_test_2d = np.array([features_2d.flatten() for features_2d in X_test])\n",
        "    pca = pca.fit(X_train_2d)\n",
        "    X_trainf = pca.transform(X_train_2d)\n",
        "    X_testf = pca.transform(X_test_2d)\n",
        "\n",
        "    Terminal = True\n",
        "    print(X_trainf.shape)\n",
        "    print(X_testf.shape)\n",
        "    \n",
        "    if Terminal:\n",
        "      print('Train Labels')\n",
        "      print(y_trainf.shape)\n",
        "      print('Train Data')\n",
        "      print(X_train.shape)\n",
        "\n",
        "    # Simple ANN Start ========================================================================================\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256,'relu', input_shape=(jj,), kernel_initializer='glorot_uniform'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512,'relu', bias_regularizer = l2(0.001) ,kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dense(512,'relu', bias_regularizer = l2(0.001) , kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256,'relu', bias_regularizer = l2(0.001) , kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dense(6))\n",
        "    model.add(Activation('Softmax'))\n",
        "\n",
        "    # . . . \n",
        "    #early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=100, restore_best_weights=True)\n",
        "    model.compile(optimizer=adam_v2.Adam(learning_rate=3e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    history = model.fit(X_trainf, y_trainf, epochs=700, validation_split=0.1, verbose = 0)#, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "    # ANN ==========================================================================================\n",
        "\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Accuracy and Loss')\n",
        "    plt.ylabel('accuracy/loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy', 'train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    score = model.evaluate(X_testf, y_testf)\n",
        "\n",
        "    y_prob= model.predict(X_testf)\n",
        "    y_pred_unseen = y_prob.argmax(axis=-1)\n",
        "\n",
        "    PRF = precision_recall_fscore_support(y_test, y_pred_unseen, average='macro')\n",
        "\n",
        "    print('\\nConfusion Matrix on Unseen Data:')\n",
        "    print(confusion_matrix(y_test, y_pred_unseen))\n",
        "\n",
        "    print('\\nReport on Unseen Data:')\n",
        "    print(classification_report(y_test, y_pred_unseen))\n",
        "\n",
        "\n",
        "    #Sourced from https://www.codegrepper.com/code-examples/python/frameworks/-file-path-python/roc+python+example\n",
        "    # probs = model.predict_proba(X_testf)\n",
        "    # preds = probs[:,1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_test, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plt.title('Receiver Operating Characteristic on Unseen Data')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "    Score_array[counter] = score[1]\n",
        "    Recall[counter] = PRF[1]\n",
        "    Precision[counter] = PRF[0] \n",
        "    F1S[counter] =  PRF[2]\n",
        "    counter = counter + 1\n",
        "\n",
        "  print(Score_array)\n",
        "\n",
        "  print(\"\\n K-Fold CV Max Test Accuracy:\")\n",
        "  print(np.max(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Min Test Accuracy:\")\n",
        "  print(np.min(Score_array))\n",
        "\n",
        "  print(\"\\n K-Fold CV Average Test Accuracy:\")\n",
        "  print(np.sum(Score_array)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Recall:\")\n",
        "  print(np.sum(Recall)/5)\n",
        "  print(\"\\n K-Fold CV Average Test Precision:\")\n",
        "  print(np.sum(Precision)/5)\n",
        "  print(\"\\n K-Fold CV Average Test F1Score:\")\n",
        "  print(np.sum(F1S)/5)\n",
        "  Whole_Array_Test[counter2][0]= jj\n",
        "  Whole_Array_Test[counter2][1]= np.max(Score_array)\n",
        "  Whole_Array_Test[counter2][2]= np.min(Score_array)\n",
        "  Whole_Array_Test[counter2][3]= np.sum(Score_array)/5\n",
        "  Whole_Array_Test[counter2][4]= np.sum(Recall)/5\n",
        "  Whole_Array_Test[counter2][5]= np.sum(Precision)/5\n",
        "  Whole_Array_Test[counter2][6]= np.sum(F1S)/5\n",
        "\n",
        "  print(Whole_Array_Test[counter2])\n",
        "    \n",
        "  counter2 = counter2 + 1\n",
        "\n",
        "print(Whole_Array_Test)\n",
        "# Results = pd.DataFrame(Whole_Array_Test)\n",
        "# Results.to_csv('Model5PCA.csv') \n",
        "# files.download('Model5PCA.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}